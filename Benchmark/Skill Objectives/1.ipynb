{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è§£æå®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼šresult.json\n",
      "âœ… æ‰€æœ‰ Q/A å¯¹å·²ä¿å­˜åˆ°ï¼šqa.json 692\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "åŠŸèƒ½ï¼šè§£æåŒ…å«å¤šä¸ªç»“æ„å—çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯å—åŒ…å« SETã€CONTENTã€QUESTIONSã€GRAPH åŒºåŸŸã€‚\n",
    "å°†å…¶è½¬æ¢ä¸º JSON æ ¼å¼ç»“æ„ï¼Œä¿å­˜åˆ° result.jsonã€‚\n",
    "\n",
    "æ–‡æœ¬ç»“æ„ç¤ºä¾‹ï¼š\n",
    "SET48\n",
    "CONTENT\n",
    "...\n",
    "QUESTIONS\n",
    "Q\n",
    "...\n",
    "A\n",
    "...\n",
    "GRAPH\n",
    "{ ... }\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "def parse_multiple_blocks(lines):\n",
    "    \"\"\"\n",
    "    è§£ææ–‡æœ¬ä¸­çš„å¤šä¸ªç»“æ„å—ï¼Œè¿”å›ä¸€ä¸ª JSON å¯¹è±¡åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = None\n",
    "\n",
    "    # çŠ¶æ€æ§åˆ¶\n",
    "    in_content = False\n",
    "    in_questions = False\n",
    "    in_graph = False\n",
    "\n",
    "    # ç¼“å­˜åŒº\n",
    "    content_buffer = []\n",
    "    question_buffer = None\n",
    "    answer_buffer = None\n",
    "    graph_buffer = []\n",
    "\n",
    "    def flush_current_block():\n",
    "        \"\"\"\n",
    "        å°†å½“å‰ç»“æ„å—ç¼“å­˜å†…å®¹å†™å…¥ blocks åˆ—è¡¨ï¼Œå¹¶æ¸…ç©ºçŠ¶æ€ã€‚\n",
    "        \"\"\"\n",
    "        nonlocal current_block, content_buffer, graph_buffer\n",
    "        nonlocal in_content, in_questions, in_graph\n",
    "\n",
    "        if current_block is None:\n",
    "            return\n",
    "\n",
    "        # å†…å®¹æ•´ç†\n",
    "        current_block[\"CONTENT\"] = \"\\n\".join(content_buffer) if content_buffer else \"\"\n",
    "\n",
    "        if graph_buffer:\n",
    "            graph_str = \"\\n\".join(graph_buffer)\n",
    "            if not graph_str.strip().startswith(\"{\"):\n",
    "                graph_str = \"{\" + graph_str + \"}\"\n",
    "            try:\n",
    "                graph_json = json.loads(graph_str)\n",
    "                current_block[\"GRAPH\"] = graph_json\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"GRAPH åŒºå— JSON è§£æå¤±è´¥ï¼š{e}\\nå†…å®¹å¦‚ä¸‹ï¼š\\n{graph_str}\")\n",
    "        else:\n",
    "            current_block[\"GRAPH\"] = {}\n",
    "\n",
    "        blocks.append(current_block)\n",
    "\n",
    "        # é‡ç½®çŠ¶æ€\n",
    "        current_block = None\n",
    "        content_buffer.clear()\n",
    "        graph_buffer.clear()\n",
    "        in_content = False\n",
    "        in_questions = False\n",
    "        in_graph = False\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # 1. æ–°å—å¼€å§‹\n",
    "        if line.startswith(\"SET\"):\n",
    "            if current_block is not None:\n",
    "                flush_current_block()\n",
    "            current_block = {\n",
    "                \"SET\": line,\n",
    "                \"CONTENT\": \"\",\n",
    "                \"QUESTIONS\": [],\n",
    "                \"GRAPH\": {}\n",
    "            }\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # å¦‚æœ SET å—æœªå¼€å§‹ï¼Œå¿½ç•¥å…¶ä»–è¡Œ\n",
    "        if current_block is None:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # åŒºå—åˆ‡æ¢é€»è¾‘\n",
    "        if line == \"CONTENT\":\n",
    "            in_content, in_questions, in_graph = True, False, False\n",
    "            i += 1\n",
    "            continue\n",
    "        elif line .startswith( \"QUESTION\"):\n",
    "            in_content, in_questions, in_graph = False, True, False\n",
    "            i += 1\n",
    "            continue\n",
    "        elif line == \"GRAPH\":\n",
    "            in_content, in_questions, in_graph = False, False, True\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # CONTENT åŒºå—é‡‡é›†\n",
    "        if in_content:\n",
    "            content_buffer.append(line)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "                # QUESTIONS åŒºå—é‡‡é›†\n",
    "        if in_questions:\n",
    "            if line == \"Q\":\n",
    "                question_lines = []\n",
    "                i += 1\n",
    "                # æ”¶é›†å¤šè¡Œ Q å†…å®¹ï¼Œç›´åˆ°é‡åˆ° A / Q / GRAPH / CONTENT\n",
    "                while i < len(lines):\n",
    "                    next_line = lines[i].strip()\n",
    "                    if next_line in {\"Q\", \"A\", \"GRAPH\", \"CONTENT\"}:\n",
    "                        break\n",
    "                    question_lines.append(next_line)\n",
    "                    i += 1\n",
    "                question_buffer = \"\\n\".join(question_lines)\n",
    "                continue\n",
    "\n",
    "            elif line == \"A\":\n",
    "                answer_lines = []\n",
    "                i += 1\n",
    "                # æ”¶é›†å¤šè¡Œ A å†…å®¹ï¼Œç›´åˆ°é‡åˆ° Q / A / GRAPH / CONTENT\n",
    "                while i < len(lines):\n",
    "                    next_line = lines[i].strip()\n",
    "                    if next_line in {\"Q\", \"A\", \"GRAPH\", \"CONTENT\"}:\n",
    "                        break\n",
    "                    answer_lines.append(next_line)\n",
    "                    i += 1\n",
    "                answer_buffer = \"\\n\".join(answer_lines)\n",
    "\n",
    "                # å­˜å‚¨é—®ç­”å¯¹\n",
    "                if question_buffer and answer_buffer:\n",
    "                    current_block[\"QUESTIONS\"].append({\n",
    "                        \"Q\": question_buffer,\n",
    "                        \"A\": answer_buffer\n",
    "                    })\n",
    "                    question_buffer = None\n",
    "                    answer_buffer = None\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # é Q/A æ ‡è¯†è¡Œï¼Œè·³è¿‡\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # GRAPH åŒºå—é‡‡é›†\n",
    "        if in_graph:\n",
    "            graph_buffer.append(line)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # å…¶ä»–é»˜è®¤è·³è¿‡\n",
    "        i += 1\n",
    "\n",
    "    # æœ€åä¸€å—è¡¥äº¤\n",
    "    if current_block is not None:\n",
    "        flush_current_block()\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path = \"./raw_text.txt\"\n",
    "    output_path = \"result.json\"\n",
    "\n",
    "    # è¯»å–å¹¶é¢„å¤„ç†\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # è§£ææ–‡æœ¬ç»“æ„\n",
    "    parsed_blocks = parse_multiple_blocks(raw_lines)\n",
    "\n",
    "    # å†™å‡ºç»“æœ JSON æ–‡ä»¶\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(parsed_blocks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… è§£æå®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼š{output_path}\")\n",
    "    # æå–æ‰€æœ‰ Q/A å¯¹å¹¶å†™å…¥ qa.json\n",
    "    all_qa_pairs = []\n",
    "    for block in parsed_blocks:\n",
    "        for qa in block.get(\"QUESTIONS\", []):\n",
    "            if \"Q\" in qa and \"A\" in qa:\n",
    "                all_qa_pairs.append({\n",
    "                    \"Q\": qa[\"Q\"],\n",
    "                    \"A\": qa[\"A\"]\n",
    "                })\n",
    "    qa_output_path = \"qa.json\"\n",
    "    with open(qa_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_qa_pairs, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… æ‰€æœ‰ Q/A å¯¹å·²ä¿å­˜åˆ°ï¼š{qa_output_path}\",len(all_qa_pairs))\n",
    "    with open(\"qa1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_qa_pairs[:350], f, ensure_ascii=False, indent=4)\n",
    "    with open(\"qa2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_qa_pairs[350:], f, ensure_ascii=False, indent=4)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²è§£æ 81 ä¸ªç—…ä¾‹ï¼Œç»“æœä¿å­˜åœ¨ parsed_cases.jsonã€‚\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# --------------------------------------------\n",
    "# Script: parse_disease_history.py\n",
    "# Purpose: è§£æ disease_history.txt æ–‡ä»¶ä¸­æ¯ä¸ªç—…ä¾‹å…ƒç´ ï¼Œæå–é¢˜å¹²ã€é¢˜ç›®ã€ç­”æ¡ˆå’Œè¯„åˆ†è¦ç‚¹ï¼Œè¾“å‡ºä¸º JSON æ ¼å¼ã€‚\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "def parse_element(text):\n",
    "    \"\"\"\n",
    "    è§£æå•ä¸ªç—…ä¾‹å…ƒç´ æ–‡æœ¬ï¼Œæå–å­—æ®µï¼š\n",
    "      - id: ç—…ä¾‹æ ‡è¯†ï¼Œå¦‚ SET51\n",
    "      - ç—…å²: BACKGROUND éƒ¨åˆ†\n",
    "      - é—®é¢˜: QUESTION éƒ¨åˆ†ï¼Œåˆ—è¡¨\n",
    "      - ç­”æ¡ˆ: ANSWER éƒ¨åˆ†ï¼Œåˆ—è¡¨\n",
    "      - è¯„åˆ†ç»†åˆ™: STANDARD éƒ¨åˆ†ï¼Œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    id_match = re.search(r\"^(SET\\d+)\", text, re.MULTILINE)\n",
    "    case_id = id_match.group(1) if id_match else None\n",
    "\n",
    "    # ç—…å²é¢˜å¹²\n",
    "    stem_match = re.search(r\"BACKGROUND\\s*(.*?)\\s*QUESTION\", text, re.S)\n",
    "    stem = stem_match.group(1).strip() if stem_match else \"\"\n",
    "\n",
    "    # é—®é¢˜\n",
    "    ques_block_match = re.search(r\"QUESTION\\s*(.*?)\\s*ANSWER\", text, re.S)\n",
    "    questions = []\n",
    "    if ques_block_match:\n",
    "        parts = re.split(r\"^QUE\", ques_block_match.group(1), flags=re.M)\n",
    "        questions = [part.strip() for part in parts if part.strip()]\n",
    "\n",
    "    # ç­”æ¡ˆ\n",
    "    ans_block_match = re.search(r\"ANSWER\\s*(.*?)\\s*STANDARD\", text, re.S)\n",
    "    answers = []\n",
    "    if ans_block_match:\n",
    "        parts = re.split(r\"^ANS\", ans_block_match.group(1), flags=re.M)\n",
    "        answers = [part.strip() for part in parts if part.strip()]\n",
    "\n",
    "    # æå–è¯„åˆ†ç»†åˆ™\n",
    "    rubric_match = re.search(r\"STANDARD\\s*(.*)\", text, re.S)\n",
    "    rubrics = []\n",
    "    if rubric_match:\n",
    "        raw = rubric_match.group(1)\n",
    "\n",
    "        # ä½¿ç”¨ lookaheadï¼Œä¿ç•™ STA å¼€å¤´ï¼Œé¿å…é¦–æ®µå‡ºé—®é¢˜\n",
    "        parts = re.split(r\"(?=^\\s*STA\\s*)\", raw, flags=re.M)\n",
    "\n",
    "        for part in parts:\n",
    "            # å»æ‰å¼€å¤´çš„ STA\n",
    "            clean = re.sub(r\"^\\s*STA\\s*\", \"\", part.strip())\n",
    "            if clean:\n",
    "                rubrics.append(clean)\n",
    "\n",
    "    # æ ¡éªŒä¸€è‡´æ€§\n",
    "    max_len = max(len(questions), len(answers), len(rubrics))\n",
    "    while len(answers) < max_len:\n",
    "        answers.append(\"\")\n",
    "    while len(rubrics) < max_len:\n",
    "        rubrics.append(\"\")\n",
    "\n",
    "    return {\n",
    "        \"id\": case_id,\n",
    "        \"ç—…å²\": stem,\n",
    "        \"é—®é¢˜\": questions,\n",
    "        \"ç­”æ¡ˆ\": answers,\n",
    "        \"è¯„åˆ†ç»†åˆ™\": rubrics\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_file(file_path):\n",
    "    \"\"\"\n",
    "    è¯»å–æ•´ä¸ªæ–‡ä»¶ï¼Œå¹¶æŒ‰ SET å¼€å¤´æ‹†åˆ†å¤šä¸ªç—…ä¾‹å…ƒç´ ï¼Œè¿”å›è§£æåçš„ JSON åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # æŒ‰ç—…ä¾‹åˆ†æ®µ (åœ¨æ¯ä¸ª SET å¼€å¤´å¤„æ‹†åˆ†)\n",
    "    raw_elements = re.split(r\"(?=^SET\\d+)\", content, flags=re.M)\n",
    "    parsed = []\n",
    "    for elem in raw_elements:\n",
    "        elem = elem.strip()\n",
    "        if not elem:\n",
    "            continue\n",
    "        parsed.append(parse_element(elem))\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ç¤ºä¾‹ï¼šè§£æ disease_history.txt å¹¶ä¿å­˜ä¸º JSON\n",
    "    cases = parse_file('disease_history.txt')\n",
    "    with open('parsed_cases.json', 'w', encoding='utf-8') as out:\n",
    "        json.dump(cases, out, ensure_ascii=False, indent=2)\n",
    "    print(f\"å·²è§£æ {len(cases)} ä¸ªç—…ä¾‹ï¼Œç»“æœä¿å­˜åœ¨ parsed_cases.jsonã€‚\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# è¯´æ˜ï¼š\n",
    "# 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æ®µå’Œæå–ã€‚\n",
    "# 2. re.S (DOTALL) ä½¿ \".\" åŒ¹é…æ¢è¡Œï¼Œre.M (MULTILINE) ä½¿ ^ å’Œ $ åŒ¹é…æ¯è¡Œã€‚\n",
    "# 3. å¯¹é¢˜ç›®éƒ¨åˆ†ä½¿ç”¨ re.split åˆ†å‰² \"QUE\" æ ‡è®°ï¼Œå¹¶æ¸…æ´—ç©ºç™½ã€‚\n",
    "# 4. è¾“å‡º JSON æ—¶ä¿æŒä¸­æ–‡å­—ç¬¦ã€‚\n",
    "# --------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ æ€»å…± 53 ä¸ª blockï¼Œ692 ä¸ªé—®é¢˜ï¼Œç›®æ ‡ä¿ç•™ 200 æ¡\n",
      "âœ… SET1: åŸå§‹ 2 â†’ ä¿ç•™ 0\n",
      "âœ… SET2: åŸå§‹ 1 â†’ ä¿ç•™ 1\n",
      "âœ… SET3: åŸå§‹ 3 â†’ ä¿ç•™ 1\n",
      "âœ… SET4: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET5: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET6: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET7: åŸå§‹ 9 â†’ ä¿ç•™ 3\n",
      "âœ… SET8: åŸå§‹ 9 â†’ ä¿ç•™ 3\n",
      "âœ… SET9: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET10: åŸå§‹ 8 â†’ ä¿ç•™ 2\n",
      "âœ… SET11: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET12: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET13: åŸå§‹ 9 â†’ ä¿ç•™ 3\n",
      "âœ… SET14: åŸå§‹ 30 â†’ ä¿ç•™ 9\n",
      "âœ… SET15: åŸå§‹ 32 â†’ ä¿ç•™ 9\n",
      "âœ… SET16: åŸå§‹ 18 â†’ ä¿ç•™ 5\n",
      "âœ… SET17: åŸå§‹ 19 â†’ ä¿ç•™ 5\n",
      "âœ… SET18: åŸå§‹ 7 â†’ ä¿ç•™ 2\n",
      "âœ… SET19: åŸå§‹ 19 â†’ ä¿ç•™ 5\n",
      "âœ… SET20: åŸå§‹ 20 â†’ ä¿ç•™ 6\n",
      "âœ… SET21: åŸå§‹ 20 â†’ ä¿ç•™ 6\n",
      "âœ… SET22: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET23: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET24: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET25: åŸå§‹ 7 â†’ ä¿ç•™ 0\n",
      "âœ… SET26: åŸå§‹ 3 â†’ ä¿ç•™ 0\n",
      "âœ… SET27: åŸå§‹ 2 â†’ ä¿ç•™ 0\n",
      "âœ… SET28: åŸå§‹ 21 â†’ ä¿ç•™ 4\n",
      "âœ… SET29: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET30: åŸå§‹ 21 â†’ ä¿ç•™ 6\n",
      "âœ… SET31: åŸå§‹ 20 â†’ ä¿ç•™ 6\n",
      "âœ… SET32: åŸå§‹ 8 â†’ ä¿ç•™ 2\n",
      "âœ… SET33: åŸå§‹ 10 â†’ ä¿ç•™ 1\n",
      "âœ… SET34: åŸå§‹ 10 â†’ ä¿ç•™ 2\n",
      "âœ… SET35: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET36: åŸå§‹ 9 â†’ ä¿ç•™ 2\n",
      "âœ… SET37: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET38: åŸå§‹ 20 â†’ ä¿ç•™ 1\n",
      "âœ… SET39: åŸå§‹ 21 â†’ ä¿ç•™ 2\n",
      "âœ… SET40: åŸå§‹ 15 â†’ ä¿ç•™ 1\n",
      "âœ… SET41: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET42: åŸå§‹ 20 â†’ ä¿ç•™ 6\n",
      "âœ… SET43: åŸå§‹ 8 â†’ ä¿ç•™ 2\n",
      "âœ… SET44: åŸå§‹ 10 â†’ ä¿ç•™ 3\n",
      "âœ… SET45: åŸå§‹ 16 â†’ ä¿ç•™ 4\n",
      "âœ… SET46: åŸå§‹ 19 â†’ ä¿ç•™ 5\n",
      "âœ… SET47: åŸå§‹ 30 â†’ ä¿ç•™ 7\n",
      "âœ… SET48: åŸå§‹ 21 â†’ ä¿ç•™ 4\n",
      "âœ… SET49: åŸå§‹ 10 â†’ ä¿ç•™ 2\n",
      "âœ… SET50: åŸå§‹ 20 â†’ ä¿ç•™ 4\n",
      "\n",
      "âœ… æœ€ç»ˆæ€»è®¡ä¿ç•™ 163 æ¡ï¼Œä¿å­˜åˆ° /home/lym/GraphRAG4OralHealth/Benchmark/Skill Objectives/basic_techniques.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import threading\n",
    "from queue import Queue\n",
    "from typing import List, Dict\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from dashscope import Generation\n",
    "\n",
    "# âœ… Qwen é…ç½®\n",
    "QWEN_API_KEY = \"sk-969f4200d53442a2a1733d1c0b1fb330\"\n",
    "QWEN_CHAT_MODEL = \"qwen-plus\"\n",
    "NUM_THREADS = 8\n",
    "\n",
    "# =====================\n",
    "# è°ƒç”¨ Qwen è¯„ä¼°æ¨¡å‹\n",
    "# =====================\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs) -> str:\n",
    "    import dashscope\n",
    "    dashscope.api_key = QWEN_API_KEY\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.extend(history_messages)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = Generation.call(\n",
    "        model=QWEN_CHAT_MODEL,\n",
    "        messages=messages,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response[\"output\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def build_evaluation_prompt(q: str, a: str) -> str:\n",
    "    return f\"\"\"\n",
    "ä½ æ˜¯ä¸€åå£è…”åŒ»å­¦è€ƒè¯•ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹é—®ç­”è¿›è¡Œç­›é€‰ï¼š\n",
    "\n",
    "é—®é¢˜ï¼š{q}\n",
    "å›ç­”ï¼š{a}\n",
    "\n",
    "è¯·åˆ¤æ–­ï¼š\n",
    "1. æ˜¯å¦æ˜¯å£è…”åŒ»å­¦é¢†åŸŸç‰¹æœ‰çš„é—®é¢˜ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n",
    "2. æ˜¯å¦æ¶‰åŠå…·ä½“çš„æ“ä½œæŠ€èƒ½ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n",
    "3. ç­”æ¡ˆæ˜¯å¦ä¸“ä¸šã€å‡†ç¡®ã€å®Œæ•´ï¼Œä½“ç°è¾ƒé«˜çš„çŸ¥è¯†æ·±åº¦ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n",
    "4. ç»¼åˆåˆ¤æ–­è¯¥é—®ç­”æ˜¯å¦é€‚åˆä½œä¸ºå£è…”é¢†åŸŸæŠ€èƒ½è¯„æµ‹é¢˜ç›®ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n",
    "\n",
    "è¯·ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹ JSON è¿”å›ï¼š\n",
    "{{\n",
    "  \"é¢†åŸŸç‰¹å¼‚æ€§\": \"æ˜¯/å¦\",\n",
    "  \"æŠ€èƒ½å¯¼å‘\": \"æ˜¯/å¦\",\n",
    "  \"ç­”æ¡ˆè´¨é‡\": \"æ˜¯/å¦\",\n",
    "  \"æ˜¯å¦æ¨è\": \"æ˜¯/å¦\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def gpt_evaluate(q: str, a: str) -> bool:\n",
    "    prompt = build_evaluation_prompt(q, a)\n",
    "    try:\n",
    "        response = llm_model_func(prompt)\n",
    "        # æå–ç¬¬ä¸€ä¸ªåˆæ³• JSON å¯¹è±¡\n",
    "        match = re.search(r'\\{.*?\\}', response, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON object found in response.\")\n",
    "        json_str = match.group(0)\n",
    "        result = json.loads(json_str)\n",
    "        return result.get(\"æ˜¯å¦æ¨è\") == \"æ˜¯\"\n",
    "    except Exception as e:\n",
    "        print(f\"[âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥] {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Worker å¤šçº¿ç¨‹å¤„ç† Q&A\n",
    "# =====================\n",
    "def qa_worker(queue: Queue, result_dict: Dict[str, List[Dict]]):\n",
    "    while not queue.empty():\n",
    "        block_id, qa = queue.get()\n",
    "        q, a = qa.get(\"Q\"), qa.get(\"A\")\n",
    "        if not q or not a:\n",
    "            queue.task_done()\n",
    "            continue\n",
    "        if gpt_evaluate(q, a):\n",
    "            result_dict.setdefault(block_id, []).append(qa)\n",
    "        queue.task_done()\n",
    "\n",
    "# =====================\n",
    "# ä¸»å‡½æ•°ï¼šè¯»å–ã€æŒ‰æ¯”ä¾‹ç­›é€‰ã€ä¿å­˜\n",
    "# =====================\n",
    "def threaded_blockwise_filter(input_path: str, output_path: str, total_target: int = 200):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        all_blocks = json.load(f)\n",
    "\n",
    "    total_questions = sum(len(b[\"QUESTIONS\"]) for b in all_blocks)\n",
    "    print(f\"ğŸ“˜ æ€»å…± {len(all_blocks)} ä¸ª blockï¼Œ{total_questions} ä¸ªé—®é¢˜ï¼Œç›®æ ‡ä¿ç•™ {total_target} æ¡\")\n",
    "\n",
    "    # ç»„ç»‡ä»»åŠ¡é˜Ÿåˆ—ï¼ˆä»¥é—®é¢˜ä¸ºå•ä½ï¼‰\n",
    "    qa_queue = Queue()\n",
    "    block_map = {}  # ç”¨äºè®°å½• block_id -> åŸå§‹ QA æ•°é‡\n",
    "\n",
    "    for block in all_blocks:\n",
    "        block_id = block.get(\"SET\", \"UNKNOWN\")\n",
    "        questions = block.get(\"QUESTIONS\", [])\n",
    "        block_map[block_id] = len(questions)\n",
    "        for qa in questions:\n",
    "            qa_queue.put((block_id, qa))\n",
    "\n",
    "    # ç”¨äºä¿å­˜æ¯ä¸ª block çš„æ¨è QA\n",
    "    result_dict = {}\n",
    "    threads = []\n",
    "\n",
    "    for _ in range(NUM_THREADS):\n",
    "        t = threading.Thread(target=qa_worker, args=(qa_queue, result_dict))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    qa_queue.join()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # ============================\n",
    "    # æ¯ä¸ª block å–å¯¹åº”æ¯”ä¾‹\n",
    "    # ============================\n",
    "    final_output = []\n",
    "    for block_id, original_count in block_map.items():\n",
    "        keep_count = max(1, round(original_count * total_target / total_questions))\n",
    "        selected = result_dict.get(block_id, [])[:keep_count]\n",
    "        final_output.extend(selected)\n",
    "        print(f\"âœ… {block_id}: åŸå§‹ {original_count} â†’ ä¿ç•™ {len(selected)}\")\n",
    "\n",
    "    print(f\"\\nâœ… æœ€ç»ˆæ€»è®¡ä¿ç•™ {len(final_output)} æ¡ï¼Œä¿å­˜åˆ° {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# =====================\n",
    "# å…¥å£å‡½æ•°\n",
    "# =====================\n",
    "if __name__ == '__main__':\n",
    "    threaded_blockwise_filter(\n",
    "        input_path=\"/home/lym/GraphRAG4OralHealth/Benchmark/Skill Objectives/raw_data.json\",\n",
    "        output_path=\"/home/lym/GraphRAG4OralHealth/Benchmark/Skill Objectives/basic_techniques.json\",\n",
    "        total_target=200\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
