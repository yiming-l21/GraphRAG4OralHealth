import json
import os
import glob
from inference_wrappers import (
    deepseek_inference, XunfeiSpark_inference, QWenplus_inference,
    ChatGLM_inference, GPT4_inference, GPT3_inference, QWen25_7b_inference,
    deepseek_r1_inference, DentalMind_o1_inference, DentalMind_base_inference,DentalMind_graph_inference
)

# ------------ æ¨¡å‹é…ç½®åŒº ------------
model_config_dict = {
    "deepseek": {"model": deepseek_inference},
    "XunfeiSpark": {"model": XunfeiSpark_inference},
    "QWenplus": {"model": QWenplus_inference},
    "ChatGLM": {"model": ChatGLM_inference},
    "GPT4turbo": {"model": GPT4_inference},
    "GPT3turbo": {"model": GPT3_inference},
    "QWen25_7b": {"model": QWen25_7b_inference},
    "deepseek_r1": {"model": deepseek_r1_inference},
    "DentalMind_o1": {"model": DentalMind_o1_inference},
    "DentalMind_base": {"model": DentalMind_base_inference},
    "DentalMind_graph":{"model": DentalMind_graph_inference}
}

# é€‰æ‹©æ¨¡å‹åç§°
model_name = "DentalMind_graph"
model_inference = model_config_dict[model_name]["model"]

# ä¿å­˜è·¯å¾„
save_dir = "/home/lym/GraphRAG4OralHealth/Benchmark/Knowledge Objectives/Answer"

# ------------ ç»“æœä¿å­˜å‡½æ•° ------------
def save_result(result_dict, topic, json_path):
    file_name = os.path.basename(json_path)
    model_folder = os.path.join(save_dir, model_name)
    os.makedirs(model_folder, exist_ok=True)
    topic_folder = os.path.join(model_folder, topic)
    os.makedirs(topic_folder, exist_ok=True)
    save_path = os.path.join(topic_folder, f"{model_name}_{topic}_{file_name}_answers.json")
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(result_dict, f, ensure_ascii=False, indent=4)

# ------------ ä¸»æ¨ç†å‡½æ•°ï¼ˆå•çº¿ç¨‹ï¼‰ ------------
from tqdm import tqdm

async def get_answer(directory_path, model_inference, topic):
    async def process_question(ques_text):
        response_content, reasoning_content = await model_inference(ques_text)
        print(ques_text)
        print(response_content)
        return response_content, reasoning_content
    all_json_paths = glob.glob(os.path.join(directory_path, '*.json'))
    for json_path in all_json_paths:
        result_list=[]
        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)

        # ç‹¬ç«‹é¢˜
        for question in json_data.get('ç‹¬ç«‹é¢˜', []):
            ques_text = (
                "ä¸‹é¢æˆ‘ä¼šç»™ä½ ä¸€ä¸ªåŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œè¯·ä½ æ ¹æ®åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
                "ä¸€ä¸ªé¢˜ç›®æœ‰5ä¸ªé€‰é¡¹ï¼Œè¯·é€‰å‡ºæœ€åˆé€‚çš„é€‰é¡¹ï¼Œå¹¶è¾“å‡ºé€‰é¡¹å‰å­—æ¯ã€‚"
                "æ³¨æ„åªéœ€è¦è¾“å‡ºé€‰é¡¹å‰å­—æ¯ï¼Œä¸éœ€è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚\n"
                f"é¢˜ç›®:\n{question.get('é¢˜å¹²', question.get('é¢˜ç›®'))}\né€‰é¡¹:\n{question['é€‰é¡¹']}"
            )
            res_content, res_reasoning =await process_question(ques_text)
            result_list.append({
                "é¢˜ç›®": question.get('é¢˜å¹²', question.get('é¢˜ç›®')),
                "é€‰é¡¹": question['é€‰é¡¹'],
                "gt": question['ç­”æ¡ˆ'],
                "prediction": res_content,
                "reason": res_reasoning
            })

        # å…±ç”¨é¢˜å¹²é¢˜
        for question in json_data.get('å…±ç”¨é¢˜å¹²é¢˜', []):
            for idx in range(len(question['ç­”æ¡ˆ'])):
                ques_text = (
                    "ä¸‹é¢æˆ‘ä¼šç»™ä½ ä¸€ä¸ªåŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œè¯·ä½ æ ¹æ®åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
                    "ä¸€ä¸ªé¢˜ç›®æœ‰5ä¸ªé€‰é¡¹ï¼Œè¯·é€‰å‡ºæœ€åˆé€‚çš„é€‰é¡¹ï¼Œå¹¶è¾“å‡ºé€‰é¡¹å‰å­—æ¯ã€‚"
                    "æ³¨æ„åªéœ€è¦è¾“å‡ºé€‰é¡¹å‰å­—æ¯ï¼Œä¸éœ€è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚\n"
                    f"é¢˜ç›®:\n{question['å…±ç”¨é¢˜å¹²']}\n{question['é¢˜å¹²'][idx]}\né€‰é¡¹:\n{question['é€‰é¡¹'][idx]}"
                )
                res_content, res_reasoning = process_question(ques_text)
                result_list.append({
                    "é¢˜ç›®": question['é¢˜å¹²'][idx],
                    "é€‰é¡¹": question['é€‰é¡¹'][idx],
                    "gt": question['ç­”æ¡ˆ'][idx],
                    "prediction": res_content,
                    "reason": res_reasoning
                })

        # å…±ç”¨å¤‡é€‰é¢˜
        for question in json_data.get('å…±ç”¨å¤‡é€‰é¢˜', []):
            for idx in range(len(question['ç­”æ¡ˆ'])):
                ques_text = (
                    "ä¸‹é¢æˆ‘ä¼šç»™ä½ ä¸€ä¸ªåŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œè¯·ä½ æ ¹æ®åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
                    "ä¸€ä¸ªé¢˜ç›®æœ‰5ä¸ªé€‰é¡¹ï¼Œè¯·é€‰å‡ºæœ€åˆé€‚çš„é€‰é¡¹ï¼Œå¹¶è¾“å‡ºé€‰é¡¹å‰å­—æ¯ã€‚"
                    "æ³¨æ„åªéœ€è¦è¾“å‡ºé€‰é¡¹å‰å­—æ¯ï¼Œä¸éœ€è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚\n"
                    f"é¢˜ç›®:\n{question['é¢˜å¹²'][idx]}\né€‰é¡¹:\n{question['é€‰é¡¹']}"
                )
                res_content, res_reasoning = process_question(ques_text)
                result_list.append({
                    "é¢˜ç›®": question['é¢˜å¹²'][idx],
                    "é€‰é¡¹": question['é€‰é¡¹'],
                    "gt": question['ç­”æ¡ˆ'][idx],
                    "prediction": res_content,
                    "reason": res_reasoning
                })
        print(result_list[:3])
        save_result(result_list, topic, json_path)



import os
import asyncio  # åˆ«å¿˜äº†å¼•å…¥ asyncio

async def main():
    topic_list = ["MedicalHumanity", "Clinical", "Dentistry", "Medical"]
    directory_path = "/home/lym/GraphRAG4OralHealth/Benchmark/Knowledge Objectives/"

    for topic in topic_list:
        path = os.path.join(directory_path, topic)
        await get_answer(path, model_inference, topic)

    print("âœ… æ‰€æœ‰é¢˜ç›®å¤„ç†å®Œæ¯•ï¼")

# ------------ è¿è¡Œå…¥å£ ------------
if __name__ == "__main__":
    asyncio.run(main())

